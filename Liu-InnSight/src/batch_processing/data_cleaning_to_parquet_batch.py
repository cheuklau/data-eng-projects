from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
import sys

"""
This Spark job reads raw csv data from AWS S3, cleans the data and then stores
it in Parquet back in AWS S3.
"""

# Create Spark Session
spark = SparkSession.builder.appName('DataCleaning').getOrCreate()

# Set up Parquet parameter
mode = "append"
parquet_file_name = 's3n://airbnbdataset/masterparquet/master.parquet'

# Create user defined function
# Removes $ and commas
get_price_udf = udf(lambda x: float(x.replace('$', '').replace(',', '')), FloatType())

# List of columns we are interested in
selected_column_name_list = ['id', 'last_scraped', 'name', 'listing_url', 'city', 'zipcode', 'price',
                             'property_type', 'bedrooms', 'accommodates', 'number_of_reviews',
                             'review_scores_rating', 'reviews_per_month']

# Grab the city name from the argument list
city = sys.argv[1]

# Open the list of links to S3 raw data.
# This list is generated by s3_urls_generation.sh
fh = open("s3_file_urls.txt", "r")
file_list = fh.read().splitlines()
# Clean each file one at a time
for file in file_list:
    try:
        # Read in the csv file
        # file will have the form s3:// to infer reading from AWS S3
        # header=True tells Spark to use the first row as dataframe column names
        # inferSchema=True tells Spark to infer what the column types are; otherwise all Strings
        # multiline=True tells Spark that certain records may have a newline character
        # escape='"' tells Spark to escape double quotes
        raw_df = spark.read.csv(file, header=True, inferSchema=True, multiLine=True, escape='"')
        # Only select the columns of interest from the dataframe
        df_with_selected_cols = raw_df.select(selected_column_name_list)
        # Drop records with missing zipcode or price columns
        df_filter_na = df_with_selected_cols.na.drop(subset=['zipcode', 'price'])
        # Drop records with null zipcode
        df_filter_null = df_filter_na.filter(df_filter_na['zipcode'].isNotNull())
        # Cast zipcode column as integer
        df_zipcode_formatted = df_filter_null.withColumn('zipcode', df_filter_null['zipcode'].cast(IntegerType()))
        # Reformat price column using previous user-defined function
        df_price_formatted = df_zipcode_formatted.withColumn('price', get_price_udf(df_zipcode_formatted['price']))
        # Rename last_scraped column as timestamp
        df_tsp_renamed = df_price_formatted.withColumnRenamed('last_scraped', 'timestamp')
        # Create month column based on timestamp
        df_with_month = df_tsp_renamed.withColumn('month', month(df_tsp_renamed['timestamp']))
        # Create date column based on timestamp
        df_final = df_with_month.withColumn('date', df_with_month['timestamp'].cast('date'))
        # Show the dataframe
        df_final.show()

        # Write final dataframe to parquet
        # Note the write.mode.parquet method of the Spark dataframe object
        df_final.write.mode(mode).parquet(parquet_file_name)

    except Exception as e:
        print(f"Unable to clean ${file}. Exception:\n ${e}")
